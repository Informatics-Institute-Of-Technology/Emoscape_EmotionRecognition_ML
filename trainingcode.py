# -*- coding: utf-8 -*-
"""TrainingCode.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1FTi6PQ8h8IBFfLMpTexBlAy2GZtkXjtF
"""

import keras
from keras.preprocessing.image import ImageDataGenerator
from keras.models import Sequential
from keras.layers import Dense, Dropout, Activation, Flatten, BatchNormalization
from keras.layers import Conv2D, MaxPooling2D
import os
from keras.optimizers import RMSprop, SGD, Adam

#!unzip /content/drive/MyDrive/Dataset/train.zip
#!unzip /content/drive/MyDrive/Dataset/validation.zip

# num of classes or emotions
num_classes=5

# size of the input image
img_rows, img_cols =48,48

# num of images supplied to the neural network per epoch
batch_size=16

train_data_dir='/content/train'
validation_data_dir ='/content/validation'

# augmenting the images to increase the dataset and increase the accuracy when trained
# images are rotated, zoomed, flpped , height and width altered
train_datagen= ImageDataGenerator(rescale=1./255,
                                  rotation_range=30, 
                                  shear_range=0.3, 
                                  zoom_range=0.3, 
                                  width_shift_range=0.4, 
                                  height_shift_range=0.4, 
                                  horizontal_flip=True, 
                                  vertical_flip=True)

validation_datagen=ImageDataGenerator(rescale=1./255)

# generating or preparing the data to feed to the neural network for training by providing the image size, image color mode , shuffling the images
train_generator=train_datagen.flow_from_directory(train_data_dir,
                                                  color_mode='grayscale',
                                                  target_size=(img_rows,img_cols),
                                                  batch_size=batch_size,
                                                  class_mode='categorical',
                                                  shuffle=True)

# generating or preparing the data for validation to check the accuracy of the model while it is being trained
validation_generator=validation_datagen.flow_from_directory(validation_data_dir,
                                                  color_mode='grayscale',
                                                  target_size=(img_rows,img_cols),
                                                  batch_size=batch_size,
                                                  class_mode='categorical',
                                                  shuffle=True)

model = Sequential()

#1st convolution layer
model.add(Conv2D(32,(3,3),padding='same',kernel_initializer='he_normal',input_shape=(img_rows,img_cols,1)))

# relu activation function will return the input directly if it is positive, otherwise it will return zero (images contains numeric pixel values)
model.add(Activation('relu'))

# distribution of the inputs to layers may change after each mini-batch when the weights are updated, batch normalisation standardizes the inputs to a layer for each mini-batch.
# batch normalization stabilizes the learning process.
model.add(BatchNormalization())

# max pooling down samples the input by calculating and returning the maximum value in a patch.
# highlights the most present feature and returns it.
model.add(MaxPooling2D(pool_size=(2,2)))

model.add(BatchNormalization())


#2nd convolution layer
#increased the number of neurons for accuracy
model.add(Conv2D(64,(3,3),padding='same',kernel_initializer='he_normal'))
model.add(Activation('relu'))
model.add(BatchNormalization())
model.add(MaxPooling2D(pool_size=(2,2)))
model.add(BatchNormalization())

#3rd convolution layer
#increased the number of neurons for accuracy
model.add(Conv2D(128,(3,3),padding='same',kernel_initializer='he_normal'))
model.add(Activation('relu'))
model.add(BatchNormalization())
model.add(MaxPooling2D(pool_size=(2,2)))
model.add(BatchNormalization())
#dropout to prevent overfitting
model.add(Dropout(0.4))

#4th convolution layer
#increased the number of neurons for accuracy
model.add(Conv2D(256,(3,3),padding='same',kernel_initializer='he_normal'))
model.add(Activation('relu'))
model.add(BatchNormalization())
model.add(MaxPooling2D(pool_size=(2,2)))
model.add(BatchNormalization())
#dropout to prevent overfitting
model.add(Dropout(0.4))

#5th convolution layer
#increased the number of neurons for accuracy
model.add(Conv2D(512,(3,3),padding='same',kernel_initializer='he_normal'))
model.add(Activation('relu'))
model.add(BatchNormalization())
model.add(MaxPooling2D(pool_size=(2,2)))
model.add(BatchNormalization())
#dropout to prevent overfitting
model.add(Dropout(0.4))

# flattening
# convert all the pooled layers into a single column  
model.add(Flatten())

# fully connected layer
# all neurons are connected to each other, each neuron in the dense layer receives input from all neurons of its previous layer
model.add(Dense(256, activation='relu'))
model.add(BatchNormalization())

# dense to the number of classes or emotions, to get the predictions or probabilities for each class.
# Softmax calculates a probability for each class.
model.add(Dense(num_classes, activation='softmax'))

print(model.summary())

#Compliling the model
model.compile(loss='categorical_crossentropy',
              optimizer='adam',
              metrics=['accuracy'])

nb_train_samples = 24282
nb_validation_samples = 5937
epochs = 50
batch_size = 16
num_classes = 5


#Training the model
model.fit_generator(
    train_generator,
    steps_per_epoch=nb_train_samples // batch_size,
    epochs=epochs,
    validation_data=validation_generator,
    validation_steps=nb_validation_samples // batch_size)


#Saving the model
model.save('TrainedModel_20_04_2021.h5')